---
layout:     post
title:      "[译] 数据压缩概述（一）"
subtitle:   "本篇无算法。"
date:       2022-02-18 12:00:00
author:     "Shicong Liu"
header-img: "img/home-dark.jpg"
catalog: true
mathjax: true
comment: true
tags:
    - 算法
    - 图像
    - 学习
    - 摸鱼
---



翻译一本外国人[^matt]关于数据压缩的专著。作者已经允许非商业性质的任意复制与传播，且在公开网站[^book]允许免费下载全书。最近没什么正事想做，闲着也是闲着，学学数据压缩好了。

# 一、信息论

> 其实是作为引言，随便翻译翻译

数据压缩是减少存储或传输数据所需的位数的艺术，可以是无损的也可以是有损的。

无损压缩的数据可以精确地解压缩到其原始值，一个典型的例子是1848年的摩斯电码[^morse]。摩斯电码中，26字母表中的每个字母都被编码为一系列点和横线，而其中，直观地，英文中最常用的字母`E`和`T`被编码了最短的`·`和`-`码字，相比之下最少见的字母如`J`，`X`，`Z`等被编码了最长四个符号的码字（分别为`·---`,`-··-`和`--··`），以便实现更好的压缩比。无损数据压缩算法通常至少包含一个模型和一个编码器。模型用于评估数据的概率分布（例如前面所说的字母出现概率等），而编码器则将较短的符号编码分配给更有可能出现的数据。

有损压缩则通常会丢弃一些「不重要」的数据，例如人的感官难以感知的数据。一个典型的例子是1953年用于广播彩色电视的NTSC标准（不少理工科专业院校还会讲授），该标准持续使用到了2009年，压缩的基本原理就是利用了人眼对颜色（如红色和绿色）之间的精细细节的敏感度低于对亮度（黑色和白色）的敏感度，因此可以调制在更窄的频带上传输。有损压缩中最重要的是区分「重要」数据与「不重要」的数据，而这个问题的描述比较抽象。我们甚至可以说这个问题是一个AI问题，因为它需要了解人脑能够感知和不能感知的内容。

信息论对可以无损压缩和不能无损压缩的内容以及压缩程度设置了严格的限制：

1. 没有「通用」压缩算法可以保证压缩任何输入，甚至任何超过一定大小的输入。特别是，不可能压缩随机数据或递归压缩。
2. 给定输入数据的概率分布，最佳的编码方式是使用$\log_2 1/p$比特编码概率为$p$的符号。
3. 所有数据都有一个确定的可压缩的最短形式，但是我们却很难确定任意字串的最短压缩结果是多少。【原文有修改】

<br>

## 1.1 压缩不是普遍的

只有少量的数据是可以被压缩的。这个一条主要出自柯尔莫哥洛夫复杂度（`Kolmogorov complexity`）[^kolmogorov]，有兴趣可以在wiki上阅读。一个对象的柯尔莫哥洛夫复杂度，例如一段文本，描述为一个计算机程序完整表示该文本的最短长度。该定义在不同领域有不同的描述，但是在简单的信息熵中，我们抛开计算机程序的概念，一段文本的柯尔莫哥洛夫复杂度就是**描述完整文本所需的最短数据长度**。

如果我们定义一段字符串的描述为$d(s)$，那么柯尔莫哥洛夫复杂度$K(s)$表示为

<div>
$$
\begin{equation}
K(s) = \min \vert d(s) \vert
\end{equation}
$$
</div>
如果一个字符串`s`能够被$\vert s \vert-c$长度的字符串描述，那我们就说字符串`s`可以被压缩$c$长度，或者$K(s)\leq \vert s \vert-c$。一个字符串不能被压缩$1$长度意味着他已经得到了最小压缩，我们称其为不可压缩串。根据鸽巢原理[^pigeonhole]，由于每一个压缩后的字符串都对应唯一的压缩前的字符串，所以不可压缩串一定存在。因为长度为$n$的字符串有$2^n$个，但是只存在$2^n-1$个长度小于$n$的字符串。


>   鸽巢原理又被称为狄利克雷抽屉原理，即将$kn+1$只鸽子放在$n$个笼子中，至少有一个笼子至少有$k+1$只鸽子。因为压缩后的子串必然更短，因此每一个压缩前长度为$n$的$2^n$个子串必然要在总计$2^n-1$的子串中寻找压缩后的位置，以此类推，因此必然存在不可压缩的子串，而可压缩的子串必然只是少量的。

基于同样的理由，大部分的字符串都是难以被显著地压缩，它们的$K(s)$并不比，$\vert s \vert$的长度小多少。准确地说，对于任意的$n$，有$2^n$个长度为$n$的字符串。我们假设这些样本呈离散均匀分布，那每一个子串出现的概率均为$2^{-n}$。这个样本空间中，一个串不能被压缩$c$的概率至少为$1-2^{-c+1}+2^{-n}$，因为长度不超过$n-c$的子串共有$2^{n-c+1}-1$个，剩余$2^{n}-2^{n-c+1}-1$个子串并不能被压缩，对于单个子串的概率我们除以总量即可得到。

<br>

## 1.2 编码有界

我们用一个简单的例子来引入这个观点。假设我们要压缩$\pi$，取前面几位有$3.14159265358979323846264...$，那么假设每个字符出现的先验概率是独立且均等的$p=0.1$，则有以下几种方式进行压缩

| Digit         | BCD   | Huffman | Binary |
| ------------- | ----- | ------- | ------ |
| 0             | 0000  | 000     | 0      |
| 1             | 0001  | 001     | 1      |
| 2             | 0010  | 010     | 10     |
| 3             | 0011  | 011     | 11     |
| 4             | 0100  | 100     | 100    |
| 5             | 0101  | 101     | 101    |
| 6             | 0110  | 1100    | 110    |
| 7             | 0111  | 1101    | 111    |
| 8             | 1000  | 1110    | 1000   |
| 9             | 1001  | 1111    | 1001   |
| bits per char | $4.0$ | $3.4$   | -      |

我们容易发现，采用`BCD`编码时，我们可以选择$4\ {\rm bit}$来表示一个字符，这时候译码器每次读取四个字符就可以表示$\pi$。如果我们用计算机中的`ASCII`码对比，这种方法压缩了$50\%$的空间。当然`BCD`并不是极限，如表格中的霍夫曼`Huffman`编码所示，计算机每次读入一个$\rm bit$，然后进行码字查表（一般采用树结构），一旦查到了对应码字就将其译码并进行重新读取。通过这种操作压缩比例进一步提升到了$57.5\%$。相比之下计算机并不能合理地处理二进制直接编码（表格最右列），因为读入码字难以相互区分。

当然霍夫曼编码也并不是极限，实际上我们可以采用霍夫曼编码来编码字符对。两位十进制数能表示为$100$个字符对，用$6\ {\rm bit}$来进行编码$0\sim 27$，用$7\ {\rm bit}$编码$28\sim 99$，我们得到平均码长$6.72$，压缩比例可以进一步提升到$58\%$。随着码字的增长，例如三位十进制码进行编码，这个结果可以提升到$58.4\%$。

根据香农的编码理论，我们可以对概率为$p$的符号编码长度为$\log_2 1/p$的码长，这个结果最高可以提升到$58.476\%$。香农将随机变量$X$的期望信息量或「不确定性」（现在通常被称为**熵**）定义为期望码长。假设随机变量$X$可以取值$X_1,X_2,\cdots,X_N$，每一个$X_i$取值概率均为$p_i$（即$P(X=X_i)=p_i$），则$X$的熵定义为

<div>
  $$
  \begin{equation}
  \begin{aligned}
  H(X)&=\mathbb{E}\left[ \log_2\left( \frac{1}{p(X)} \right) \right]\\
  &=\sum_{i=1}^N p_i\log_2\left( \frac{1}{p_i} \right)
  \end{aligned}
  \end{equation}
  $$
</div>
我们以$\pi$为例，想要压缩编码十进制$\pi$，最低也需要$10(0.1*\log_2(1/0.1))=3.3219\ {\rm bit}$，没有任何无损的方法能够低于这个约束。此外，信息量（或「不确定性」）是可以叠加的，其联合信息量最高是他们信息量的和。例如两组字符串$X,Y$的联合信息量为$H(X,Y)\leq H(X)+H(Y)$，取等条件为两者完全独立（知道其中一方完全不可以推断另一方）。

另外还有条件熵的定义，即$H(X\vert Y)=H(X,Y)-H(Y)$。他的含义是当给定$Y$时$X$的信息量，如果二者独立，显然$H(X\vert Y)=H(X)$。

回到字符串压缩中，如果随机变量$X$可以表示成字符串$x_1,x_2,\cdots,x_N$，那么$X$的分布又可以表示成如下形式[^chainrule1]

<div>
  $$
  \begin{equation}
  \begin{aligned}
  p(X)=\prod_{i=1}^N p(x_i \vert x_1\cdots x_{i-1})
  \end{aligned}
  \end{equation}
  $$
</div>

那么与之对应的就是$X$的信息量

<div>
  $$
  \begin{equation}
  \begin{aligned}
  H(X)=\sum_{i=1}^N H(x_i \vert x_1\cdots x_{i-1})
  \end{aligned}
  \end{equation}
  $$
</div>
熵既可以说是「不确定性」的度量，又可以说是压缩后期望码长的下界。基于熵的编码方式（熵编码）有许多种，均为无损压缩的方法，但是这里需要强调的是「熵」的数值仅在完全已知先验分布的条件下才能精确算得，但一般来说对于一个自然界中的现象，我们往往很难知道他的确切概率分布。

<br>

## 1.3 最短压缩不可达

上一段中我们采用均匀分布的假设对$\pi$进行压缩编码。在此模型假设下，香农的编码理论给了我们一个可达最小压缩的硬限制。然而，我们仍然有可能采用一个更优的模型，因为$\pi$的每一位并不是随机变量。这些数值仅在我们不主动计算的条件下是未知的，但是如果我们的智能压缩器识别到了压缩目标是$\pi$，我们仍可以建立一个规则，例如`$\pi$的前一百万位`这种描述（类似我们前面所说的**柯尔莫哥洛夫复杂度**）。基于香农的理论，我们将一百万位的$\pi$压缩后最小可以得到$415,241 {\rm bytes}$的大小，但是我们仍可以采用短至$52\ {\rm bytes}$的程序[^52bytes]来计算它.

上文我们提到，绝大多数字符串是不可压缩的，然而一个相当显著的事实却是，我们所关心的大部分数字字符串、图像、软件等实际上都是可以压缩的。他们都具有比自身长度更为简短的描述。

对于不同语言的柯尔莫哥洛夫复杂度我们不再赘述，但时至今日我们仍然无法证明存在一个程序可以计算任意字符串$s$的柯氏复杂度$K(s)$。除此之外，我们无法对一个串的随机性做出合理的判断，也无法证明一个任意串是否可以被进一步压缩。我们可以做一个简单的“诡辩”，对于$\pi$的前一百万位我们已经可以做简短描述，但是如果我们采用AES加密，在密钥未知的条件下密文看起来就是完全随机的。即便其中存在很高的冗余性，哪怕我们加密的内容是全零，密文仍然难以观测到充分的冗余。

<br>

## *1.4 完备的压缩是一个AI问题


> 这一段作者的诠释比较抽象，大家可以自己去网站体验原文。站在2022的视角我不能完全同意作者的思想，但或许只是我没能完全理解，因此不做翻译。
>
> 这篇文章最后修改于2013年，大部分内容成文于2007年，而2015年的人工智能热潮之后人们才逐渐开始深化序列预测、图像重建等问题。本质上这些应用其实都是对信息论的一种探索。

<!--

预测直观上与理解是相互关联的，如果我们完全理解一个序列的规律，那我们就可以预测它后续的走向；如果我们完全理解一个语言，我们就有机会预测某一个段落或一个空白处该填入什么单词；如果我们完全理解一个图像，我们就可以预测被掩盖的部分是什么。相反的是，随机数据不可理解且难以预测。这个直觉表明压缩和预测过程可以用来测试或衡量算法对目标的理解。

> 这篇文章最后修改于2013年，大部分内容成文于2007年，而2015年的人工智能热潮之后人们才逐渐开始深化序列预测、图像重建等问题。本质上这些应用其实都是对信息论的一种探索。

最后这里我们引入几个很常见的小例子。如果我们认为最优（或者完备）压缩是可计算的，那势必会解决两种目前最常见的智能测试问题，即图灵测试[^turing]和通用智能测试[^ua]。

很明显，上下文语句问答问题可以建模为$P(A\vert Q)=P(AQ)/P(Q)$的模型，其中$Q$是问答环节的上下文，$A$则是回复。对于给定问题，机器作出答复即可通过图灵测试，而如果一个模型能够通过图灵测试，那他一定可以完美地解决这个问答模型问题，至少可以产生与人类没有区别的反应，例如`$\pi$的前一百位`这种答复。这种预测很困难，而且需要大量的外部知识。香农用人类受试者在n-gram频率表和字典的辅助下猜测连续词汇，估计无标点符号的书面不区分大小写英语的信息量为每个字符$0.6$到$1.3\ {\rm bit}$。「不确定性」与其说是由于主题与人类技能的变化，不如说是由于不同的概率分配导致相同的观察到的猜测序列。尽管如此，最好的文本压缩器现在也只是在这个范围的上限附近进行压缩。

> 这句不是太懂

通用智能的定义更类似于强化学习那套理论，它考虑随机程序描述的完全任意的环境内一个寻求智能体寻求奖励最大化的过程。智能体通过收发符号与环境通信，其中环境会向智能体发送奖励信号。通用智能的定义是在所有可能的环境中获得的奖励期望。

-->

<br>

## 1.5 小结

- 没有通用的、递归的或随机数据的压缩算法。
- 尽管大部分字符串都是随机的，但是有意义的字符串往往不是（是可压缩的）。
- 压缩即概率分布建模与编码过程。编码容易，但概率建模过程是证明不可解的。
- 压缩是一门艺术，可以说是一个人工智能的问题。压缩的关键是了解/理解被压缩的数据。

<br>



# 二、评测基准

差点忘记我们要讲压缩算法了。数据压缩的评测基准一般而言都是指在数据集上的压缩比，偶尔也会提及在指定计算机上的内存使用以及压缩速率。为了避免硬件依赖的因素的影响，有的基准测试只会评价大小。但是总而言之，就是

- 压缩比
- 内存使用量
- 压缩速率

本章中我们来介绍一下基本的压缩评价方法和常用的压缩评价数据库，需要看算法的可以跳过此段。

## 2.1 Calgary语料库

`Calgary`语料库是仍在使用的几乎最古老的压缩基准语料库了，由14个文件组成，总大小为3,141,622字节，如下所示：

```
    111,261 BIB    - 725篇文献引用记录（UNIX refer格式，ASCII）
    768,771 BOOK1  - Thomas Hardy: 远离尘嚣（未排版，ASCII）
    610,856 BOOK2  - Witten: 计算机语音原理（UNIX troff排版，ASCII）
    102,400 GEO    - 地震资料（32 bit IBM标准浮点）
    377,109 NEWS   - USENET各种主题新闻（ASCII）
     21,504 OBJ1   - 程序编译结果（VAX可执行程序）
    246,814 OBJ2   - Mac可执行程序 - 知识支持系统
     53,161 PAPER1 - Witten, Neal, Cleary: 数据压缩的算术编码（UNIX troff排版）
     82,199 PAPER2 - Witten: 计算机安全（UNIX troff排版）
    513,216 PIC    - 1728 x 2376 bitmap image (MSB first，书页内容为法语与图表)
     39,611 PROGC  - C语言源码（UNIX compress v4.0压缩）
     71,646 PROGL  - Lisp语言源码（系统软件）
     49,379 PROGP  - Pascal语言源码（PPM压缩检测程序）
     93,695 TRANS  - 终端会话记录（ASCII，控制符）
```

语料库的结构如下图所示，每个像素代表连续出现的字符串之间的匹配，像素的颜色代表匹配的长度：黑色代表1个字节，红色代表2个字节，绿色代表4个字节，蓝色代表8个字节。横轴代表语料样本，纵轴表示某个词汇重复出现时的距离。举例，在`BOOK1`数据集中我们清晰可见约60-70的位置有一条黑色的线，这个线是周期出现的换行符导致的。`GEO`语料库里面的1280行和5576行的黑线则是块数据表头导致的。图像数据在1处的黑色代表了图像中存在大量的0。上方的蓝色分布则代表了一些词汇的反复出现。如此种种简单描述了数据的结构分布。由于数据结构的各有不同，因此将其分别压缩才能实现更好的效果。

<img align="middle" src="/img/in-post/compress/calgary.gif" height="75%" width="75%"/>

<br>

## 2.2 大规模文本压缩基准测试

现在的大规模文本压缩基准[^large]文件是单个`Unicode`编码`XML`文件，包含`2006/03/03`一来的维基百科文本转储，截断至$10^9 {\rm bytes}$，旨在鼓励人工智能，尤其是自然语言处理的而研究。

<img align="middle" src="/img/in-post/compress/e9.png" height="75%" width="75%"/>

<br>

## 2.3 Hutter奖

`Hutter`奖基于第一个长度为$10^8\ {\rm bytes}$的语料库压缩竞赛设立，这个比赛中每提升$1\%$的性能将获得500欧元的奖励，但总性能增益需要超过$3\%$才可颁发，内存与运行时间消耗也局限于测试机。

> 顺带一提目前性能最好的是2009年5月23日的Alexander Rhatushnyak的提交，结果是15,949,688 bytes，压缩耗时7608秒，占用936MB内存。处理器为2 GHz dual core T3200，运行环境32 bit Windows Vista。
>
> 用于对比的基线算法为zip -9压缩命令，压缩结果为36,445,373 bytes，耗时3.5秒，占用0.1MB内存。

<br>

## 2.4 最大压缩基准测试

maximum compression benchmark[^maximum]由两部分组成，10个公开数据集总计53MB，一个私有数据集共计510个文件301MB。公开数据集分别压缩并计算压缩结果，最终排名只评估压缩后数据大小。公开的十个文件如下

```
     842,468 a10.jpg      - 高质量1152 x 864 JPEG战斗机图像
   3,870,784 acrord32.exe - x86 可执行文件 - Acrobat Reader 5.0
   4,067,439 english.dic  - 一个按照字母表排序的英文字典，共354,941词汇
   4,526,946 FlashMX.pdf  - 内嵌JPEG和压缩BMP图像的PDF文档
  20,617,071 fp.log       - web服务器日志（ASCII）
   3,782,416 mso97.dll    - x86 来自Microsoft Office的动态链接库
   4,168,192 ohs.doc      - 内嵌JPEG的word文档（word文档本身就是zip结构）
   4,149,414 rafale.bmp   - 1356 x 1020 16 位深度彩色图像，保存为24 bit RGB
   4,121,418 vcfiu.hlp    - OCX 帮助文档（二进制数据，内嵌文字）
   2,988,578 world95.txt  - 1995 CIA 世界概况手册（ASCII）
```

数据很随意。

<br>

## 2.5 通用压缩基准测试

通用压缩基准[^generic]不发布任何测试数据，测试思路也是按照之前的通用人工智能方案，那一段我没有翻译。详情可以参考[^ua][^AIXI]等。内部测试数据由一个私密的种子产生，包含一百万个随机图灵机输出的比特串，阶段为256比特长度后打包成以`null`结尾的字符串，平均大小约为6.5MB。该测试允许公开验证，并且不需要测量解压缩大小。测试流程是基本可重现的，误差在$0.05\%$左右。程序被通过压缩比例排名，

> 由于我本人目前并不熟悉相关知识，关于通用智能与此对应压缩基准的问题我会后续跟进

<br>

## 2.6 压缩排名基准测试

Compression Ratings[^CR]压缩基准测试数据共有5.4GB，数据包括英文文本、Windows 可执行代码、RGB 和灰度图像、CD 质量的音频以及来自两个游戏的混合类型数据文件。对比的基准算法是tar压缩结果，压缩大小包括解压程序源代码或用7zip压缩的可执行程序的大小。该测试会对比运行时间，所有参赛程序必须在一个小数据集上达到入门成绩才能参与后续对比。

<br>

## 2.7 其他&小结

类似的基准测试还有很多，例如Squeeze Chart[^SC]，Monster of Compression[^MOC]，UCLC[^UCLC]，Metacompressor[^MC]，Xtreme Compression[^XC]等。

在数据压缩的热衷上我似乎看到了现在Kaggle的一些痕迹。技术的进步不仅需要依靠项目推动，还需要像压缩公开赛一样的促进与推广。目前，通信技术中也有许多项目在利用竞赛推进，虽然这些项目短期内还没有看到落地或标准化的希望，但我真心希望新的技术能借此机会逐渐渗透到我们未来的生活中。

本章写的很随意，是我在日常摸鱼时间里随便翻译的，一些比较抽象的内容等我有大块时间之后再去仔细研究一下。另外热身部分就此结束，后面内容我会简单介绍一些常见的压缩算法和相关应用，如果能采用目前程序复现我也会尽力尝试。

`2022-02-20`



<br>


---


[^matt]:[Matt Mahoney's Home Page](http://mattmahoney.net/l)

[^book]:[Data Compression Explained](http://mattmahoney.net/dc/dce.html)
[^morse]:[Morse Code](https://en.wikipedia.org/wiki/Morse_code)
[^kolmogorov]:[Kolmogorov complexity](https://en.wikipedia.org/wiki/Kolmogorov_complexity)
[^pigeonhole]:[Pigeonhole Principle](https://en.wikipedia.org/wiki/Pigeonhole_principle)
[^chainrule1]:[Chain Rule (probability)](https://en.wikipedia.org/wiki/Chain_rule_(probability))
[^52bytes]:[52 bytes for pi](http://www.boo.net/~jasonp/pipage.html)
[^turing]:[Turing Test](http://en.wikipedia.org/wiki/Turing_test)
[^ua]:[Universal Intelligence](http://en.wikipedia.org/wiki/Universal_Intelligence#Use_in_artificial_intelligence)
[^AIXI]:[AIXI](http://hutter1.net/ai/uaibook.htm)
[^large]:[Large Text Compression Benchmark](http://mattmahoney.net/dc/text.html)
[^maximum]:[maximum compression benchmark](http://www.maximumcompression.com/)
[^generic]:[Generic Compression Benchmark](http://mattmahoney.net/dc/uiq/)
[^CR]:[Compression Ratings](http://compressionratings.com/)
[^SC]:[Squeeze Chart](http://www.squeezechart.com/)
[^MOC]:[Monster of Compression](http://heartofcomp.altervista.org/MOC/MOC.htm)
[^UCLC]:[UCLC](http://uclc.info/)
[^MC]:[Metacompressor](http://metacompressor.com/)
[^XC]:[Xtreme Compression](http://www.xtremecompression.com/compare.html)
